[General]
# Possible levels (must be UC-only): CRITICAL, ERROR, WARNING, INFO, DEBUG
# default: DEBUG
loglevel = DEBUG
# logformat, see https://docs.python.org/2/library/logging.html#logrecord-attributes
# default: [%(pathname)s:%(lineno)d] %(message)s
logformat = [%(pathname)s:%(lineno)d] %(message)s



[Crawler]

# Following Strings in the SavePath will be replaced (time will be replaced with the current time at download, not execution of the script):
#
# %time(<code>) = will be replaced with strftime(<code>) where <code> is a string, explained further here: http://strftime.org/
# %timestamp    = current unix-timestamp
# %domain       = the domain of the crawled file (e.g. zeit.de)
# %full_domain  = the domain including subdomains (e.g. panamapapers.sueddeutsche.de)
# %directory_string = The directories on the server (e.g. http://panamapapers.sueddeutsche.de/articles/56f2c00da1bb8d3c3495aa0a/ would evaluate to articles_56f2c00da1bb8d3c3495aa0a), no filename
# %file_name    = The file name (without type) on the server (e.g. http://www.spiegel.de/wirtschaft/soziales/ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466.html would evaluate to ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466, No filenames (indexes) will evaluate to index
#
# default: "%full_domain/%directory_string/%file_name_%timestamp.html"

SavePath = %full_domain/%directory_string/%file_name_%timestamp.html

# Crawling heuristics
# possible "recursive" and "sitemap"
# array, default: ["recursive", "sitemap"]

CrawlingHeuristics = ["recursive", "sitemap"]

# TODO: Choose what heuristics for article recognition should be used
# Either Booleans, or a list of heuristics that should be applied

# Respect Robots.txt
# default: True
RespectRobots = True

# Allow Cookies
# default: True
AllowCookies = True

[Files]
# In this section you can specify the input and output-JSON-Files

# The input-file file containing the base-urls to crawl
# absolute and relative file paths to initial.py are allowed
URLInput = ./input_data.json

# The output-file containing all the meta-information about the files
# Allowed replacements:
# %time(<code>) (Will be the time at the execution)
Output = ../output.json


[ScrapyOptions]

# Here you can set any scrapy options from the scrapy docs!
# http://doc.scrapy.org/en/latest/topics/settings.html#built-in-settings-reference
# TODO: Write out relevant, working settings
