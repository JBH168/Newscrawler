[General]

# Paths: toggle relative to initial.py (True)
# or relative to this config file (False)
RelativeToInitial = True

[Crawler]

# Following Strings in the SavePath will be replaced: (md5 hashes have a standard length of 32 chars)
#
# %time_download(<code>)                  = current time at download; will be replaced with strftime(<code>) where <code> is a string, explained further here: http://strftime.org/
# %time_execution(<code>)                 = current time at execution; will be replaced with strftime(<code>) where <code> is a string, explained further here: http://strftime.org/
# %timestamp_download                     = current time at download; unix-timestamp
# %timestamp_execution                    = current time at execution; unix-timestamp
# %domain(<size>)                         = first <size> chars of the domain of the crawled file (e.g. zeit.de)
# %appendmd5_domain(<size>)               = appends the md5 to %domain(<<size> - 32 (md5 length) - 1 (_ as separator)>) if domain is longer than <size>
# %md5_domain(<size>)                     = first <size> chars of md5 hash of %domain
# %full_domain(<size>)                    = first <size> chars of the domain including subdomains (e.g. panamapapers.sueddeutsche.de)
# %appendmd5_full_domain(<size>)          = appends the md5 to %full_domain(<<size> - 32 (md5 length) - 1 (_ as separator)>) if full_domain is longer than <size>
# %md5_full_domain(<size>)                = first <size> chars of md5 hash of %full_domain
# %subdomains(<size>)                     = first <size> chars of the domain's subdomains
# %appendmd5_subdomains(<size>)           = appends the md5 to %subdomains(<<size> - 32 (md5 length) - 1 (_ as separator)>) if subdomains is longer than <size>
# %md5_subdomains(<size>)                 = first <size> chars of md5 hash of %subdomains
# %url_directory_string(<size>)           = first <size> chars of the directories on the server (e.g. http://panamapapers.sueddeutsche.de/articles/56f2c00da1bb8d3c3495aa0a/ would evaluate to articles_56f2c00da1bb8d3c3495aa0a), no filename
# %appendmd5_url_directory_string(<size>) = appends the md5 to %url_directory_string(<<size> - 32 (md5 length) - 1 (_ as separator)>) if url_directory_string is longer than <size>
# %md5_url_directory_string(<size>)       = first <size> chars of md5 hash of %url_directory_string(<size>)
# %url_file_name(<size>)                  = first <size> chars of the file name (without type) on the server (e.g. http://www.spiegel.de/wirtschaft/soziales/ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466.html would evaluate to ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466, No filenames (indexes) will evaluate to index
# %md5_url_file_name(<size>)              = first <size> chars of md5 hash of %url_file_name
# %max_url_file_name                      = first x chars of %url_file_name, so the entire savepath has a length of the max possible length for a windows file system (260 characters - 1 <NUL>)
# %appendmd5_max_url_file_name            = appends the md5 to the first x - 32 (md5 length) - 1 (_ as separator) chars of %url_file_name if the entire savepath has a length longer than the max possible length for a windows file system (260 characters - 1 <NUL>)
#
# default: ./data/%time_execution(%Y)/%time_execution(%m)/%time_execution(%d)/%appendmd5_domain(32)/%appendmd5_subdomains(32)_%appendmd5_url_directory_string(60)_%appendmd5_max_url_file_name.html
SavePath = ./data/%time_execution(%Y)/%time_execution(%m)/%time_execution(%d)/%appendmd5_full_domain(32)/%appendmd5_url_directory_string(60)_%appendmd5_max_url_file_name_%timestamp_download.html


# Crawling heuristics
# Default Crawlers:
# Possibilities: recursiveCrawler, recursiveSitemapCrawler, rssCrawler, sitemapCrawler (./newscrawler/crawler/spiders/-dir)
# Default: recursiveCrawler
Default = recursiveCrawler

# hrefs are ignored for recursive crawling if they end on any of the following file extensions
ignoreFileExtensions = "(pdf)|(docx?)|(xlsx?)|(pptx?)|(epub)|(jpe?g)|(png)|(bmp)|(gif)|(tiff)|(webp)|(avi)|(mpe?g)|(mov)|(qt)|(webm)|(ogg)|(midi)|(mid)|(mp3)|(wav)|(zip)|(rar)|(exe)|(apk)|(css)"

# Crawl on sitemaps of subdomains (if sitemap is enabled)
# default: True
SitemapAllowSubdomains = True

# Number of crawlers, that should crawl parallel
# not counting in daemonized crawlers
NumberOfParallelCrawlers = 5

# Number of daemons, will be added to daemons.
NumberOfParallelDaemons = 10

# Determines the allowed age difference for the RSS crawler
deltatime = 6

fallbacks = {
    "rssCrawler": None,
    "recursiveSitemapCrawler": "recursiveCrawler",
    "sitemapCrawler": "recursiveCrawler",
    "recursiveCrawler": None
  }

# The crawler module containing all crawlers.
# Crawlers in the crawler-module must have the same name as the class in them.
Module = newscrawler.crawler.spiders

[Heuristics]
# TODO: Choose what heuristics for article recognition should be used

# Enabled heuristics,
# Currently:
#    - og_type
#    - linked_headlines
#    - self_linked_headlines
#    - is_not_from_subdomain
# (maybe not up-to-date, see ./newscrawler/helper_classes/heursitics.py:
#  Every method not starting with __ should be a heuristic, except is_article)
# These heuristics can be overwritten by input_data.json for every site
enabled_heuristics = {"og_type": True, "linked_headlines": "<=0.5", "self_linked_headlines": "<=0.3"}

pass_heuristics_condition = "og_type and self_linked_headlines and linked_headlines"

# The maximum ratio of headlines divided by linked_headlines in a file

# The minimum number of headlines in a file to check for the ratio
# If less then this number are in the file, the file will pass the test.
min_headlines_for_linked_test = 5


[Files]
# In this section you can specify the input and output-JSON-Files

# The input-file file containing the base-urls to crawl
# absolute and relative file paths to initial.py are allowed
URLInput = ./input_data.json



[Database]

# MySQL-Connection required for saving meta-informations
host = db.dbvis.de
port = 3306
db = ccolon
username = ccolon
password = b3eY7Tep2F7Pg559Vg0W



[Scrapy]

# Possible levels (must be UC-only): CRITICAL, ERROR, WARNING, INFO, DEBUG
# default: DEBUG
LOG_LEVEL = DEBUG

# logformat, see https://docs.python.org/2/library/logging.html#logrecord-attributes
# default: [%(name)s:%(lineno)d|%(levelname)s] %(message)s
LOG_FORMAT = [%(name)s:%(lineno)d|%(levelname)s] %(message)s

# Can be a filename or None
# default: None
LOG_FILE = None

LOG_DATEFORMAT = %Y-%m-%d %H:%M:%S

LOG_STDOUT = False

LOG_ENCODING = utf-8


BOT_NAME = 'newscrawler'

SPIDER_MODULES = ['newscrawler.crawler.spiders']
NEWSPIDER_MODULE = 'newscrawler.crawler.spiders'

## Resume/Pause functionality activation
## JOBDIR is always relative to the dir the script initial.py is called from
## except it's an absolute path
JOBDIR = ./.resume_jobdir

## Respect robots.txt activation
## (Default: False)
ROBOTSTXT_OBEY=True

## Maximum number of concurrent requests across all domains
## (Default: 16)
CONCURRENT_REQUESTS=16

## Maximum number of active requests per domain
CONCURRENT_REQUESTS_PER_DOMAIN=4

## User-agent activation
USER_AGENT = 'ccolon_newscrawler (+http://www.uni-konstanz.de)'

## Pipeline activation
## Syntax: '<relative location>.<Pipeline name>': <Order of execution from 0-1000>
ITEM_PIPELINES = {'newscrawler.crawler.pipelines.RSSCrawlCompare':300, 'newscrawler.crawler.pipelines.LocalStorage':400, 'newscrawler.crawler.pipelines.DatabaseStorage': 500}
