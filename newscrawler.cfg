[General]

# Possible levels (must be UC-only): CRITICAL, ERROR, WARNING, INFO, DEBUG
# default: DEBUG
loglevel = DEBUG

# logformat, see https://docs.python.org/2/library/logging.html#logrecord-attributes
# default: [%(name)s:%(lineno)d|%(levelname)s] %(message)s
logformat = [%(name)s:%(lineno)d|%(levelname)s] %(message)s



[Crawler]

# Following Strings in the SavePath will be replaced: (md5 hashes have a standard length of 32 chars)
#
# %time_download(<code>)        = current time at download; will be replaced with strftime(<code>) where <code> is a string, explained further here: http://strftime.org/
# %time_execution(<code>)       = current time at execution; will be replaced with strftime(<code>) where <code> is a string, explained further here: http://strftime.org/
# %timestamp_download           = current time at download; unix-timestamp
# %timestamp_execution          = current time at execution; unix-timestamp
# %domain                       = the domain of the crawled file (e.g. zeit.de)
# %md5_domain(<size>)           = first <size> chars of md5 hash of %domain
# %full_domain                  = the domain including subdomains (e.g. panamapapers.sueddeutsche.de)
# %url_directory_string(<size>) = first <size> chars of the directories on the server (e.g. http://panamapapers.sueddeutsche.de/articles/56f2c00da1bb8d3c3495aa0a/ would evaluate to articles_56f2c00da1bb8d3c3495aa0a), no filename
# %url_file_name(<size>)        = first <size> chars of the file name (without type) on the server (e.g. http://www.spiegel.de/wirtschaft/soziales/ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466.html would evaluate to ttip-dokumente-leak-koennte-ende-der-geheimhaltung-markieren-a-1090466, No filenames (indexes) will evaluate to index
# %md5_url_file_name(<size>)    = first <size> chars of md5 hash of %url_file_name
# %max_url_file_name            = first x chars of %url_file_name, so the entire savepath has a length of the max possible length for a windows file system (260 characters - 1 <NUL>)
#
# default: ./data/%time_execution(%y)/%time_execution(%m)/%time_execution(%d)/%domain/%max_url_file_name.html
SavePath = ./data/%time_execution(%y)/%time_execution(%m)/%time_execution(%d)/%domain/%max_url_file_name.html

# Crawling heuristics
Sitemap = True
RecursiveSitemap = False

# Crawl on sitemaps of subdomains (if sitemap is enabled)
# default: True
SitemapAllowSubdomains = True



[Heuristics]
# TODO: Choose what heuristics for article recognition should be used

og_type_article = True



[Files]
# In this section you can specify the input and output-JSON-Files

# The input-file file containing the base-urls to crawl
# absolute and relative file paths to initial.py are allowed
URLInput = ./input_data.json

# The output-file containing all the meta-information about the files
# Allowed replacements:
# %time(<code>) (Will be the time at the execution)
Output = ../output.json



[Database]

# MySQL-Connection required for saving meta-informations

host = db.dbvis.de
port = 3306
db = ccolon
username = ccolon
password = b3eY7Tep2F7Pg559Vg0W



[Scrapy]

BOT_NAME = 'newscrawler'

SPIDER_MODULES = ['newscrawler.crawler.spiders']
NEWSPIDER_MODULE = 'newscrawler.crawler.spiders'

## Scrapy's built in logging
## Disabled due to a bug which displayes log information twice
LOG_ENABLED = False

## Resume/Pause functionality activation
JOBDIR = ./.resume_jobdir

## Respect robots.txt activation
## (Default: False)
ROBOTSTXT_OBEY=True

## Maximum number of concurrent requests across all domains
## (Default: 16)
#CONCURRENT_REQUESTS=32

## Maximum number of active requests per domain
CONCURRENT_REQUESTS_PER_DOMAIN=8

## User-agent activation
USER_AGENT = 'ccolon_newscrawler (+http://www.uni-konstanz.de)'

## Pipeline activation
## Syntax: '<relative location>.<Pipeline name>': <Order of execution from 0-1000>
ITEM_PIPELINES = {
	'newscrawler.pipeline.Heuristics': 200,
	'newscrawler.pipelines.DatabaseStorage': 300,
	'newscrawler.pipelines.LocalStorage': 400
}
